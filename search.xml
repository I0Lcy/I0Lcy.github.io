<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>python爬虫并发</title>
    <url>/2021/02/13/python%E7%88%AC%E8%99%AB%E5%B9%B6%E5%8F%91/</url>
    <content><![CDATA[<h2 id="并发下载"><a href="#并发下载" class="headerlink" title="并发下载"></a>并发下载</h2><h3 id="多线程和多进程补充知识点"><a href="#多线程和多进程补充知识点" class="headerlink" title="多线程和多进程补充知识点"></a>多线程和多进程补充知识点</h3><h4 id="threading-local类"><a href="#threading-local类" class="headerlink" title="threading.local类"></a>threading.local类</h4><p>使用线程时最不愿意遇到的情况就是多个线程竞争资源，在这种情况下为了保证资源状态的正确性，我们可能需要对资源进行加锁保护的处理，这一方面会导致程序失去并发性，另外如果多个线程竞争多个资源时，还有可能因为加锁方式的不当导致<a href="https://zh.wikipedia.org/wiki/%E6%AD%BB%E9%94%81">死锁</a>。要解决多个线程竞争资源的问题，其中一个方案就是让每个线程都持有资源的副本（拷贝），这样每个线程可以操作自己所持有的资源，从而规避对资源的竞争。</p>
<p>要实现将资源和持有资源的线程进行绑定的操作，最简单的做法就是使用<code>threading</code>模块的<code>local</code>类，在网络爬虫开发中，就可以使用<code>local</code>类为每个线程绑定一个MySQL数据库连接或Redis客户端对象，这样通过线程可以直接获得这些资源，既解决了资源竞争的问题，又避免了在函数和方法调用时传递这些资源。具体的请参考本章多线程爬取“手机搜狐网”（Redis版）的实例代码。</p>
<h4 id="concurrent-futures模块"><a href="#concurrent-futures模块" class="headerlink" title="concurrent.futures模块"></a>concurrent.futures模块</h4><p>Python3.2带来了<code>concurrent.futures</code> 模块，这个模块包含了线程池和进程池、管理并行编程任务、处理非确定性的执行流程、进程/线程同步等功能。关于这部分的内容推荐大家阅读<a href="http://python-parallel-programmning-cookbook.readthedocs.io/zh_CN/latest/index.html">《Python并行编程》</a>。</p>
<h4 id="分布式进程"><a href="#分布式进程" class="headerlink" title="分布式进程"></a>分布式进程</h4><p>使用多进程的时候，可以将进程部署在多个主机节点上，Python的<code>multiprocessing</code>模块不但支持多进程，其中<code>managers</code>子模块还支持把多进程部署到多个节点上。当然，要部署分布式进程，首先需要一个服务进程作为调度者，进程之间通过网络进行通信来实现对进程的控制和调度，由于<code>managers</code>模块已经对这些做出了很好的封装，因此在无需了解网络通信细节的前提下，就可以编写分布式多进程应用。具体的请参照本章分布式多进程爬取“手机搜狐网”的实例代码。</p>
<h3 id="协程和异步I-O"><a href="#协程和异步I-O" class="headerlink" title="协程和异步I/O"></a>协程和异步I/O</h3><h4 id="协程的概念"><a href="#协程的概念" class="headerlink" title="协程的概念"></a>协程的概念</h4><p>协程（coroutine）通常又称之为微线程或纤程，它是相互协作的一组子程序（函数）。所谓相互协作指的是在执行函数A时，可以随时中断去执行函数B，然后又中断继续执行函数A。注意，这一过程并不是函数调用（因为没有调用语句），整个过程看似像多线程，然而协程只有一个线程执行。协程通过<code>yield</code>关键字和 <code>send()</code>操作来转移执行权，协程之间不是调用者与被调用者的关系。</p>
<p>协程的优势在于以下两点：</p>
<ol>
<li>执行效率极高，因为子程序（函数）切换不是线程切换，由程序自身控制，没有切换线程的开销。</li>
<li>不需要多线程的锁机制，因为只有一个线程，也不存在竞争资源的问题，当然也就不需要对资源加锁保护，因此执行效率高很多。</li>
</ol>
<blockquote>
<p><strong>说明</strong>：协程适合处理的是I/O密集型任务，处理CPU密集型任务并不是它擅长的，如果要提升CPU的利用率可以考虑“多进程+多线程”或者“多进程+协程”的工作模式。</p>
</blockquote>
<h4 id="历史回顾"><a href="#历史回顾" class="headerlink" title="历史回顾"></a>历史回顾</h4><ol>
<li>Python 2.2：第一次提出了生成器（最初称之为迭代器）的概念（PEP 255）。</li>
<li>Python 2.5：引入了将对象发送回暂停了的生成器这一特性即生成器的<code>send()</code>方法（PEP 342）。</li>
<li>Python 3.3：添加了<code>yield from</code>特性，允许从迭代器中返回任何值（注意生成器本身也是迭代器），这样我们就可以串联生成器并且重构出更好的生成器。</li>
<li>Python 3.4：引入<code>asyncio.coroutine</code>装饰器用来标记作为协程的函数，协程函数和<code>asyncio</code>及其事件循环一起使用，来实现异步I/O操作。</li>
<li>Python 3.5：引入了<code>async</code>和<code>await</code>，可以使用<code>async def</code>来定义一个协程函数，这个函数中不能包含任何形式的<code>yield</code>语句，但是可以使用<code>return</code>或<code>await</code>从协程中返回值。</li>
</ol>
<p>协程实现了协作式并发，通过提高CPU的利用率来达到改善性能的目的。著名的三方库<a href="https://github.com/aio-libs/aiohttp"><code>aiohttp</code></a>就是通过协程的方式实现了HTTP客户端和HTTP服务器的功能，较之<code>requests</code>有更好的获取数据的性能，有兴趣可以阅读它的<a href="https://aiohttp.readthedocs.io/en/stable/">官方文档</a>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> aiohttp</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">download</span>(<span class="params">url</span>):</span></span><br><span class="line">    print(<span class="string">&#x27;Fetch:&#x27;</span>, url)</span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession() <span class="keyword">as</span> session:</span><br><span class="line">        <span class="keyword">async</span> <span class="keyword">with</span> session.get(url, ssl=<span class="literal">False</span>) <span class="keyword">as</span> resp:</span><br><span class="line">            print(url, <span class="string">&#x27;---&gt;&#x27;</span>, resp.status)</span><br><span class="line">            print(url, <span class="string">&#x27;---&gt;&#x27;</span>, resp.headers)</span><br><span class="line">            print(<span class="string">&#x27;\n\n&#x27;</span>, <span class="keyword">await</span> resp.text())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    loop = asyncio.get_event_loop()</span><br><span class="line">    urls = [</span><br><span class="line">        <span class="string">&#x27;https://www.baidu.com&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;http://www.sohu.com/&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;http://www.sina.com.cn/&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;https://www.taobao.com/&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;http://jd.com/&#x27;</span></span><br><span class="line">    ]</span><br><span class="line">    tasks = [download(url) <span class="keyword">for</span> url <span class="keyword">in</span> urls]</span><br><span class="line">    loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line">    loop.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h3 id="实例-多线程爬取“手机搜狐网”所有页面"><a href="#实例-多线程爬取“手机搜狐网”所有页面" class="headerlink" title="实例 - 多线程爬取“手机搜狐网”所有页面"></a>实例 - 多线程爬取“手机搜狐网”所有页面</h3><p>下面我们把之间讲的所有知识结合起来，用面向对象的方式实现一个爬取“手机搜狐网”的多线程爬虫。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> zlib</span><br><span class="line"><span class="keyword">from</span> enum <span class="keyword">import</span> Enum, unique</span><br><span class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> sha1</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> threading <span class="keyword">import</span> Thread, current_thread, local</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> bson <span class="keyword">import</span> Binary</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@unique</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpiderStatus</span>(<span class="params">Enum</span>):</span></span><br><span class="line">    IDLE = <span class="number">0</span></span><br><span class="line">    WORKING = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode_page</span>(<span class="params">page_bytes, charsets=(<span class="params"><span class="string">&#x27;utf-8&#x27;</span>,</span>)</span>):</span></span><br><span class="line">    page_html = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> charset <span class="keyword">in</span> charsets:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            page_html = page_bytes.decode(charset)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">except</span> UnicodeDecodeError:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">return</span> page_html</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Retry</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, *, retry_times=<span class="number">3</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 wait_secs=<span class="number">5</span>, errors=(<span class="params">Exception, </span>)</span>):</span></span><br><span class="line">        self.retry_times = retry_times</span><br><span class="line">        self.wait_secs = wait_secs</span><br><span class="line">        self.errors = errors</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, fn</span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.retry_times):</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    <span class="keyword">return</span> fn(*args, **kwargs)</span><br><span class="line">                <span class="keyword">except</span> self.errors <span class="keyword">as</span> e:</span><br><span class="line">                    print(e)</span><br><span class="line">                    sleep((random() + <span class="number">1</span>) * self.wait_secs)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Spider</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.status = SpiderStatus.IDLE</span><br><span class="line"></span><br><span class="line"><span class="meta">    @Retry()</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fetch</span>(<span class="params">self, current_url, *, charsets=(<span class="params"><span class="string">&#x27;utf-8&#x27;</span>, </span>),</span></span></span><br><span class="line"><span class="function"><span class="params">              user_agent=<span class="literal">None</span>, proxies=<span class="literal">None</span></span>):</span></span><br><span class="line">        thread_name = current_thread().name</span><br><span class="line">        print(<span class="string">f&#x27;[<span class="subst">&#123;thread_name&#125;</span>]: <span class="subst">&#123;current_url&#125;</span>&#x27;</span>)</span><br><span class="line">        headers = &#123;<span class="string">&#x27;user-agent&#x27;</span>: user_agent&#125; <span class="keyword">if</span> user_agent <span class="keyword">else</span> &#123;&#125;</span><br><span class="line">        resp = requests.get(current_url,</span><br><span class="line">                            headers=headers, proxies=proxies)</span><br><span class="line">        <span class="keyword">return</span> decode_page(resp.content, charsets) \</span><br><span class="line">            <span class="keyword">if</span> resp.status_code == <span class="number">200</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, html_page, *, domain=<span class="string">&#x27;m.sohu.com&#x27;</span></span>):</span></span><br><span class="line">        soup = BeautifulSoup(html_page, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> a_tag <span class="keyword">in</span> soup.body.select(<span class="string">&#x27;a[href]&#x27;</span>):</span><br><span class="line">            parser = urlparse(a_tag.attrs[<span class="string">&#x27;href&#x27;</span>])</span><br><span class="line">            scheme = parser.scheme <span class="keyword">or</span> <span class="string">&#x27;http&#x27;</span></span><br><span class="line">            netloc = parser.netloc <span class="keyword">or</span> domain</span><br><span class="line">            <span class="keyword">if</span> scheme != <span class="string">&#x27;javascript&#x27;</span> <span class="keyword">and</span> netloc == domain:</span><br><span class="line">                path = parser.path</span><br><span class="line">                query = <span class="string">&#x27;?&#x27;</span> + parser.query <span class="keyword">if</span> parser.query <span class="keyword">else</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line">                full_url = <span class="string">f&#x27;<span class="subst">&#123;scheme&#125;</span>://<span class="subst">&#123;netloc&#125;</span><span class="subst">&#123;path&#125;</span><span class="subst">&#123;query&#125;</span>&#x27;</span></span><br><span class="line">                redis_client = thread_local.redis_client</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> redis_client.sismember(<span class="string">&#x27;visited_urls&#x27;</span>, full_url):</span><br><span class="line">                    redis_client.rpush(<span class="string">&#x27;m_sohu_task&#x27;</span>, full_url)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">extract</span>(<span class="params">self, html_page</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store</span>(<span class="params">self, data_dict</span>):</span></span><br><span class="line">        <span class="comment"># redis_client = thread_local.redis_client</span></span><br><span class="line">        <span class="comment"># mongo_db = thread_local.mongo_db</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpiderThread</span>(<span class="params">Thread</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name, spider</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(name=name, daemon=<span class="literal">True</span>)</span><br><span class="line">        self.spider = spider</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self</span>):</span></span><br><span class="line">        redis_client = redis.Redis(host=<span class="string">&#x27;1.2.3.4&#x27;</span>, port=<span class="number">6379</span>, password=<span class="string">&#x27;1qaz2wsx&#x27;</span>)</span><br><span class="line">        mongo_client = pymongo.MongoClient(host=<span class="string">&#x27;1.2.3.4&#x27;</span>, port=<span class="number">27017</span>)</span><br><span class="line">        thread_local.redis_client = redis_client</span><br><span class="line">        thread_local.mongo_db = mongo_client.msohu </span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            current_url = redis_client.lpop(<span class="string">&#x27;m_sohu_task&#x27;</span>)</span><br><span class="line">            <span class="keyword">while</span> <span class="keyword">not</span> current_url:</span><br><span class="line">                current_url = redis_client.lpop(<span class="string">&#x27;m_sohu_task&#x27;</span>)</span><br><span class="line">            self.spider.status = SpiderStatus.WORKING</span><br><span class="line">            current_url = current_url.decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> redis_client.sismember(<span class="string">&#x27;visited_urls&#x27;</span>, current_url):</span><br><span class="line">                redis_client.sadd(<span class="string">&#x27;visited_urls&#x27;</span>, current_url)</span><br><span class="line">                html_page = self.spider.fetch(current_url)</span><br><span class="line">                <span class="keyword">if</span> html_page <span class="keyword">not</span> <span class="keyword">in</span> [<span class="literal">None</span>, <span class="string">&#x27;&#x27;</span>]:</span><br><span class="line">                    hasher = hasher_proto.copy()</span><br><span class="line">                    hasher.update(current_url.encode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line">                    doc_id = hasher.hexdigest()</span><br><span class="line">                    sohu_data_coll = mongo_client.msohu.webpages</span><br><span class="line">                    <span class="keyword">if</span> <span class="keyword">not</span> sohu_data_coll.find_one(&#123;<span class="string">&#x27;_id&#x27;</span>: doc_id&#125;):</span><br><span class="line">                        sohu_data_coll.insert_one(&#123;</span><br><span class="line">                            <span class="string">&#x27;_id&#x27;</span>: doc_id,</span><br><span class="line">                            <span class="string">&#x27;url&#x27;</span>: current_url,</span><br><span class="line">                            <span class="string">&#x27;page&#x27;</span>: Binary(zlib.compress(pickle.dumps(html_page)))</span><br><span class="line">                        &#125;)</span><br><span class="line">                    self.spider.parse(html_page)</span><br><span class="line">            self.spider.status = SpiderStatus.IDLE</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_any_alive</span>(<span class="params">spider_threads</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">any</span>([spider_thread.spider.status == SpiderStatus.WORKING</span><br><span class="line">                <span class="keyword">for</span> spider_thread <span class="keyword">in</span> spider_threads])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">thread_local = local()</span><br><span class="line">hasher_proto = sha1()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    redis_client = redis.Redis(host=<span class="string">&#x27;1.2.3.4&#x27;</span>, port=<span class="number">6379</span>, password=<span class="string">&#x27;1qaz2wsx&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> redis_client.exists(<span class="string">&#x27;m_sohu_task&#x27;</span>):</span><br><span class="line">        redis_client.rpush(<span class="string">&#x27;m_sohu_task&#x27;</span>, <span class="string">&#x27;http://m.sohu.com/&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    spider_threads = [SpiderThread(<span class="string">&#x27;thread-%d&#x27;</span> % i, Spider())</span><br><span class="line">                      <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br><span class="line">    <span class="keyword">for</span> spider_thread <span class="keyword">in</span> spider_threads:</span><br><span class="line">        spider_thread.start()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> redis_client.exists(<span class="string">&#x27;m_sohu_task&#x27;</span>) <span class="keyword">or</span> is_any_alive(spider_threads):</span><br><span class="line">        sleep(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">&#x27;Over!&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
</search>
